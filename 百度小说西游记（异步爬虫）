'''异步爬虫（百度小说西游记）'''
import requests
import asyncio
import aiohttp
import json
import aiofiles
'''
1.同步操作：访问getCatalog 拿到所有章节的cid和名称
2.异步操作：访问getChapterContent 下载所有的文章内容
'''
async def aiodownload(cid,book_id,title):
    data={"book_id":book_id,
          "cid":f"{book_id}|{cid}",
          "need_bookinfo":1
            }
    data=json.dumps(data)
    url=f'https://dushu.baidu.com/api/pc/getChapterContent?data={data}'#请求和访问章节的cid

    async with aiohttp.ClientSession()as session:
        async with session.get(url)as resp:
            dic=await resp.json()
            async with aiofiles.open(f'/User/{title}.txt',mode='w',encoding='utf-8')as f:
                await f.write(dic['data']['novel']['content'])

async def getCatalog(url):#第一步：同步操作部分用回requests请求拿章节名称和cid
    resp=requests.get(url)
    dic=resp.json()
    tasks=[]
    for item in dic['data']['novel']['items']:#item就是对应每一个章节的名称和cid
        title=item['title']
        cid=item['cid']
        #准备异步任务
        tasks.append(aiodownload(cid,book_id,title))
    await asyncio.wait(tasks)

if __name__=='__main__':
    book_id="4306063500"
    url='https://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"'+book_id+'"}'
    asyncio.run(getCatalog(url))
